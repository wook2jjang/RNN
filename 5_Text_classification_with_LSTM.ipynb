{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e90420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7692832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a34c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 저장 directory에 데이터 다운로드 받기\n",
    "data_dir = 'data'\n",
    "file_path = f'{data_dir}/AG_news_train.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9db3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "file_path = f'{data_dir}/AG_news_test.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04089039",
   "metadata": {},
   "source": [
    "## 데이터를 열어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/AG_news_train.csv', names=['class', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1910042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede7f74",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Class ID는 1~4까지로 총 4개의 클래스가 존재</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>1: World</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>2: Sports</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>3: Business</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>4: Sci/Tech</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c9af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_file_path = 'data/AG_news_train.csv'\n",
    "tst_file_path = 'data/AG_news_test.csv'\n",
    "trn_pipe, tst_pipe = preprocess.get_pipe(trn_file_path=trn_file_path, \n",
    "                                         tst_file_path=tst_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9c419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and val datapipes early on. Will build vocabulary from training datapipe only.\n",
    "trn_dp, val_dp = trn_pipe.random_split(total_length=len(list(trn_pipe)),\n",
    "                                       weights={\"train\": 0.8, \"valid\": 0.2},\n",
    "                                       seed=0)\n",
    "\n",
    "tst_dp = tst_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f721ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " 'Carlyle Looks Toward Commercial Aerospace (Reuters)',\n",
       " 'Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trn_dp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c06f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e466d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(preprocess.preprocess_english) # 별도로 정의한 함수로 tokenizing하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8076e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = preprocess.get_vocab(trn_dp, tokenizer, data_type = 'description')\n",
    "label_vocab = preprocess.get_vocab(trn_dp, tokenizer, data_type = 'class', specials = [\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdfae58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_VALUE = text_vocab['<UNK>']\n",
    "PADDING_VALUE = text_vocab['<PAD>']\n",
    "VOCAB_SIZE = len(text_vocab.get_stoi())\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c70dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28062"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58059cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = lambda x: [text_vocab[token] for token in tokenizer(x)]\n",
    "label_transform = lambda x: int(x)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f19490dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    desc_list, label_list = [], []\n",
    "    for (_label, _title, _desc) in batch:\n",
    "        processed_desc = torch.tensor(text_transform(_desc))\n",
    "        desc_list.append(processed_desc)\n",
    "        label_list.append(label_transform(_label))     \n",
    "        \n",
    "    labels = torch.tensor(label_list).to(DEVICE)\n",
    "    descs = pad_sequence(desc_list, padding_value = PADDING_VALUE, batch_first=True).to(DEVICE)\n",
    "    \n",
    "    return labels, descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3a44e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28062"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f689527",
   "metadata": {},
   "source": [
    "## nn.Embedding() 알아보기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>정수 index가 들어오면, 그 index에 맞는 벡터를 추출해 주는 lookup table(같은 것)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f77539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4587,  1.0858,  0.2739],\n",
       "        [-0.8754, -2.6280,  0.3633],\n",
       "        [ 0.7138, -1.2440, -0.1990],\n",
       "        [ 1.8255, -2.3799, -0.0456],\n",
       "        [-0.8720, -0.7075, -0.4527],\n",
       "        [-1.4513, -0.7704,  0.2369]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=6, \n",
    "                     embedding_dim=3)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420791f0",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>[Optional] 알아보기 쉽도록 weight값을 바꿔봅시다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02126e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [2., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 2.]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([[1,0,0],[0,1,0],[0,0,1],[2,0,0],[0,2,0],[0,0,2]])\n",
    "w = torch.tensor(w, dtype=torch.float32)\n",
    "embed.weight.data = w\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd088612",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Integer Index가 들어오면 해당 index의 row vector를 반환해줍니다. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cf5ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[0., 1., 0.]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "idx1 = torch.LongTensor([0])\n",
    "idx2 = torch.LongTensor([1])\n",
    "print(embed(idx1))\n",
    "print(embed(idx2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c596efa",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Integer Sequence가 들어오면, sequence의 각 integer index에 해당하는 row vector들을 반환합니다</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>근데, 우리는 문장을 어떻게 받나요? 각 단어가 integer encoding된 상태인 integer sequence로 받습니다</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>즉, `nn.Embedding`을 사용하면 문장내의 각 단어들을 vector로 표현한 tensor로 만들 수 있어요</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d7a9a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sentence = torch.LongTensor([1,0,2])\n",
    "print(embed(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80b96f",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>심지어 여러개의 문장에 대해서도 tensor형태로 변환할 수 있습니다.</span>\n",
    "    - <span style = 'font-size:1.0em;line-height:1.5em'>첫번째 문장(integer encoding된): [1,0,2]</span>\n",
    "    - <span style = 'font-size:1.0em;line-height:1.5em'>첫번째 문장(integer encoding된): [3,5,4]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e59dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[2., 0., 0.],\n",
      "         [0., 0., 2.],\n",
      "         [0., 2., 0.]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sentences = torch.LongTensor([[1,0,2],[3,5,4]])\n",
    "print(embed(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227633d",
   "metadata": {},
   "source": [
    "# 모델 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41eccf8",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>간단한 RNN기반의 문서 분류 모델을 정의합시다. 크게 다음과 같은 component가 존재합니다.</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>Embedding layer</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>단어가 Integer Encoding된 문장 sequence가 들어오면, integer index에 맞는 vector로 변환</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>vocabulary 수(`n_vocab`)는 미정, 단어 벡터 차원(`embed_dim`) = 200</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>RNN layer</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>RNN layer 수(`n_layer`) = 1, 은닉층 차원(`hidden_dim`) = 512</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>FFNN layer</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>마지막 time에서의 hidden을 산출하면, 이를 간단한 FFNN의 입력값으로 사용</span>\n",
    "        - <span style = 'font-size:1.1em;line-height:1.5em'>FFNN의 input_dim = RNN의 hidden_dim=512, FFNN의 은닉층 = 128, FFNN의 output_dim = n_class = 4</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39d86d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim=200, n_layer=1, \n",
    "                 hidden_dim=512, output_dim=4, dropout_ratio=0.5):\n",
    "        \n",
    "        super(MyNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim, \n",
    "                           hidden_size=hidden_dim, \n",
    "                           num_layers=n_layer, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_dim, \n",
    "                             out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, \n",
    "                            out_features=output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "    def forward(self, x): # x: integer encoded sentence\n",
    "        embed = self.embedding(x)\n",
    "        outputs, (h_n, c_n) = self.rnn(embed)\n",
    "        last_output = outputs[:,-1,:]\n",
    "        out = self.fc1(last_output)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59003e",
   "metadata": {},
   "source": [
    "# train() 함수\n",
    "### (Week03의 '4_코드_정리.ipynb' 참조)\n",
    "\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`train()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득, tensor로 변환한 뒤, 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 지난 batch로부터 계산했던 gradient를 초기화(`zero_grad()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Backpropagation으로 각 parameter의 gradient를 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> Gradient Descent로 parameter값 update</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> `trn_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 8.</b> 데이터 한 개당 평균 train loss 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "257c859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (label, text) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = torch.LongTensor(text).to(device)\n",
    "        y = torch.LongTensor(label).to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = criterion(y_pred_prob, y)\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7623bb8",
   "metadata": {},
   "source": [
    "# evaluate()함수\n",
    "\n",
    "### (Week03의 '4_코드_정리.ipynb' 참조)\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`evaluate()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득, tensor로 변환한 뒤, 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> 모델이 예측하는 레이블을 산출 (with `torch.argmax()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Minibatch의 실제 레이블(`y`)과 예측 레이블(`y_pred_label`)을 누적하여 저장</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> `eval_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> 데이터 한 개당 평균 evaluation loss와 accuracy 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22e2467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (label, text) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = torch.LongTensor(text).to(device)\n",
    "            y = torch.LongTensor(label).to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = criterion(y_pred_prob, y)\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0d7b8",
   "metadata": {},
   "source": [
    "# 실제 모델 학습 및 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "002ef41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(n_vocab=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bbdad",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>학습한 모델을 저장할 directory 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d8a7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67bf47",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요한 hyperparameter값 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3359e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 2**6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ab812",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eaa5d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dp_list = list(trn_dp)\n",
    "val_dp_list = list(val_dp)\n",
    "tst_dp_list = list(tst_dp)\n",
    "trn_loader = DataLoader(trn_dp_list,\n",
    "                        batch_sampler=preprocess.BatchSamplerSimilarLength(dataset=trn_dp_list,\n",
    "                                                                           tokenizer=tokenizer,\n",
    "                                                                           batch_size=BATCH_SIZE),\n",
    "                        collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dp_list, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_batch)\n",
    "tst_loader = DataLoader(tst_dp_list, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, \n",
    "                        collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f176b",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>loss function정의하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0c38197",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d03630",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>optimizer 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adb41fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac4b88",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch에 드는 시간을 측정하는 함수</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7c2c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe128a2",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c31dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 6m 44s\n",
      "\tTrain Loss: 1.274 | Val Loss: 0.757 | Val Acc: 66.171% \n",
      "Epoch: 02 | Time: 6m 51s\n",
      "\tTrain Loss: 0.454 | Val Loss: 0.357 | Val Acc: 87.983% \n",
      "Epoch: 03 | Time: 6m 41s\n",
      "\tTrain Loss: 0.265 | Val Loss: 0.325 | Val Acc: 89.258% \n",
      "Epoch: 04 | Time: 6m 41s\n",
      "\tTrain Loss: 0.202 | Val Loss: 0.321 | Val Acc: 89.562% \n",
      "Epoch: 05 | Time: 6m 38s\n",
      "\tTrain Loss: 0.150 | Val Loss: 0.342 | Val Acc: 89.517% \n",
      "Epoch: 06 | Time: 6m 49s\n",
      "\tTrain Loss: 0.107 | Val Loss: 0.389 | Val Acc: 89.212% \n",
      "Epoch: 07 | Time: 6m 40s\n",
      "\tTrain Loss: 0.074 | Val Loss: 0.425 | Val Acc: 89.129% \n",
      "Epoch: 08 | Time: 6m 39s\n",
      "\tTrain Loss: 0.055 | Val Loss: 0.453 | Val Acc: 89.412% \n",
      "Epoch: 09 | Time: 6m 41s\n",
      "\tTrain Loss: 0.040 | Val Loss: 0.521 | Val Acc: 88.746% \n",
      "Epoch: 10 | Time: 6m 39s\n",
      "\tTrain Loss: 0.032 | Val Loss: 0.544 | Val Acc: 89.296% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     criterion=loss_func,\n",
    "                     optimizer=my_opt, \n",
    "                     device=DEVICE)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=val_loader, \n",
    "                                  criterion=loss_func,\n",
    "                                  optimizer=my_opt, \n",
    "                                  device=DEVICE)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model3.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc973e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load(f'{save_dir}/my_model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86c88a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 1.3037e+00, -1.2928e+00, -6.4955e-02,  ..., -1.2056e+00,\n",
       "                        1.6196e-02,  2.7363e-01],\n",
       "                      [-1.5426e-01,  1.0207e+00,  2.2950e+00,  ..., -2.9864e-01,\n",
       "                        7.1270e-01,  1.5385e-01],\n",
       "                      [-1.1895e-01,  2.6510e-01,  5.4087e-01,  ...,  1.4373e+00,\n",
       "                        2.4773e-02,  7.0790e-01],\n",
       "                      ...,\n",
       "                      [ 1.7491e-03,  9.0890e-01,  1.3982e+00,  ...,  3.4248e-01,\n",
       "                        4.2014e-01,  7.2376e-01],\n",
       "                      [ 5.5438e-01,  6.6477e-01,  1.2892e+00,  ..., -1.7038e-01,\n",
       "                        8.2063e-01,  1.2597e+00],\n",
       "                      [ 7.4464e-01, -1.4678e-02, -1.7131e-01,  ..., -7.4931e-01,\n",
       "                        1.4259e+00, -7.1166e-01]])),\n",
       "             ('rnn.weight_ih_l0',\n",
       "              tensor([[ 0.0542,  0.0412, -0.0433,  ..., -0.0485,  0.1257, -0.1004],\n",
       "                      [-0.0380,  0.0616,  0.0159,  ...,  0.0984,  0.0506,  0.0561],\n",
       "                      [-0.0204, -0.0410,  0.0390,  ..., -0.0027,  0.0215, -0.0201],\n",
       "                      ...,\n",
       "                      [ 0.1133,  0.0245,  0.0249,  ...,  0.0884, -0.0883, -0.0559],\n",
       "                      [-0.0560, -0.0664, -0.0505,  ..., -0.0049,  0.0130,  0.0441],\n",
       "                      [-0.0005,  0.1317, -0.0407,  ...,  0.0063, -0.0174,  0.1813]])),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[-0.0645,  0.0386,  0.0051,  ..., -0.0054, -0.0404, -0.0751],\n",
       "                      [-0.1226, -0.0634, -0.1599,  ..., -0.0206, -0.0849,  0.0495],\n",
       "                      [ 0.0122,  0.1138,  0.0637,  ..., -0.0315, -0.0993, -0.0575],\n",
       "                      ...,\n",
       "                      [-0.0303,  0.0006,  0.0252,  ...,  0.0079, -0.0395, -0.1348],\n",
       "                      [-0.0903,  0.0609, -0.0801,  ..., -0.1122, -0.0848,  0.0514],\n",
       "                      [-0.0678, -0.0351,  0.0502,  ...,  0.0199,  0.1090,  0.0366]])),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([ 0.0731,  0.0666,  0.0692,  ...,  0.0165, -0.1414,  0.0283])),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([ 0.0082,  0.0516,  0.0336,  ...,  0.0028, -0.1242, -0.0128])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0222,  0.0753,  0.0960,  ...,  0.0259, -0.0180,  0.1843],\n",
       "                      [ 0.0287, -0.0330,  0.1050,  ..., -0.0626,  0.0410, -0.0078],\n",
       "                      [ 0.0604, -0.0030,  0.0179,  ...,  0.0419, -0.0491,  0.0640],\n",
       "                      ...,\n",
       "                      [ 0.0238,  0.0818,  0.0491,  ...,  0.0861,  0.0123, -0.0360],\n",
       "                      [-0.0471,  0.0223, -0.0334,  ..., -0.0205, -0.0364, -0.0331],\n",
       "                      [ 0.0728, -0.0659,  0.0292,  ...,  0.0925, -0.0367,  0.1066]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0141,  0.0314,  0.0550,  0.0207,  0.0434, -0.0715, -0.0037,  0.0899,\n",
       "                      -0.0009, -0.0637, -0.0146, -0.0320, -0.0737, -0.0503,  0.0406, -0.0293,\n",
       "                       0.0308,  0.1164,  0.0018,  0.0059,  0.0756, -0.0070,  0.1064,  0.0322,\n",
       "                       0.0711, -0.0341,  0.0440,  0.0630,  0.0259,  0.0102, -0.0651,  0.0771,\n",
       "                       0.0930, -0.0394,  0.0522, -0.0630,  0.0187,  0.0407,  0.0005,  0.1111,\n",
       "                      -0.0640,  0.0003,  0.0269,  0.0852, -0.0264,  0.0809,  0.0487,  0.0052,\n",
       "                       0.0444,  0.0062, -0.0546, -0.0576, -0.0649,  0.0260, -0.0359, -0.0646,\n",
       "                       0.0371, -0.0210,  0.0624,  0.0146,  0.0622, -0.0351, -0.0551,  0.0802,\n",
       "                      -0.0231, -0.0033, -0.0654,  0.0329,  0.0676, -0.0930,  0.0400, -0.0676,\n",
       "                       0.1375,  0.0086, -0.0205,  0.0604, -0.0984, -0.0382,  0.0230, -0.0255,\n",
       "                       0.0687, -0.0544, -0.0844, -0.0100, -0.0186, -0.1014,  0.0120, -0.0015,\n",
       "                       0.0092,  0.0286, -0.0014,  0.0762,  0.0364,  0.0254, -0.0079, -0.0567,\n",
       "                      -0.0406,  0.0339, -0.0495, -0.0661,  0.0386, -0.0415,  0.0106,  0.0450,\n",
       "                       0.0291, -0.0194, -0.0677,  0.0264,  0.0876, -0.0524,  0.0176, -0.0367,\n",
       "                      -0.0371, -0.0441, -0.0669,  0.0822, -0.0619, -0.0432, -0.0537, -0.0283,\n",
       "                       0.0346, -0.0595, -0.0406, -0.0961, -0.0437,  0.1521, -0.0572,  0.0064])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 8.0375e-02, -4.0525e-02,  2.2405e-01,  1.3870e-01, -3.2877e-02,\n",
       "                        5.6723e-02, -3.6498e-02, -1.7108e-01,  3.8020e-02,  2.4339e-02,\n",
       "                       -1.1819e-01, -3.5355e-04,  4.5904e-03,  5.8554e-02,  2.3200e-01,\n",
       "                       -4.0634e-02, -1.0905e-02,  8.6935e-02,  1.4627e-02, -2.4383e-02,\n",
       "                       -2.3314e-03,  7.6690e-02,  1.0066e-01, -1.6614e-01,  5.1285e-02,\n",
       "                       -1.1404e-02, -3.6760e-02, -1.5944e-03, -7.1810e-02,  2.1797e-01,\n",
       "                       -3.1473e-02, -3.6913e-02,  6.6990e-03, -3.8734e-02, -1.5779e-01,\n",
       "                       -9.2819e-02, -1.3548e-01,  2.0916e-01, -7.3192e-03,  4.7270e-02,\n",
       "                        2.6860e-02, -7.7392e-02, -1.1758e-01,  1.1103e-01,  8.0845e-02,\n",
       "                        2.0921e-01, -2.8887e-02, -6.2801e-02, -1.2814e-01,  6.4822e-02,\n",
       "                       -5.7850e-02, -3.3556e-02,  4.0458e-02, -1.0282e-01, -6.5726e-02,\n",
       "                       -2.9332e-02, -4.6653e-02, -3.4263e-02,  2.5213e-01, -3.6614e-02,\n",
       "                       -1.3010e-01,  3.0100e-02,  5.0576e-02, -1.2296e-01, -2.3595e-01,\n",
       "                       -9.0533e-02,  4.9679e-02,  2.6782e-03, -1.8142e-01, -2.1203e-02,\n",
       "                       -1.2562e-01, -4.5176e-02,  6.2563e-03, -1.0845e-01,  1.1208e-01,\n",
       "                        7.2997e-02,  3.0759e-03,  7.6329e-02,  1.5043e-01, -8.0130e-02,\n",
       "                        2.8446e-02, -9.1560e-02,  4.7975e-03, -1.3210e-02, -2.2218e-01,\n",
       "                       -4.1086e-03, -5.8406e-02, -6.3545e-02,  1.2447e-01, -1.8182e-01,\n",
       "                        4.6717e-02,  6.1319e-03,  1.7575e-01, -8.4903e-02, -6.1403e-02,\n",
       "                       -1.8202e-02, -6.5851e-02, -1.3685e-01, -2.9559e-02,  1.6486e-03,\n",
       "                        8.3228e-02, -7.9502e-02,  5.7081e-03,  1.0868e-01,  5.5869e-02,\n",
       "                       -1.3256e-01,  1.4235e-02,  2.5026e-02,  4.7696e-02, -3.4980e-02,\n",
       "                       -1.6658e-01, -1.5235e-02, -1.1141e-01,  1.0569e-01,  5.5135e-02,\n",
       "                        1.1844e-01, -9.3480e-02, -5.5772e-02, -1.2028e-01, -6.7229e-02,\n",
       "                       -1.7623e-01,  2.4186e-02,  3.0086e-02, -1.4608e-01, -6.5069e-02,\n",
       "                        1.6779e-01,  1.0174e-03,  1.8806e-02],\n",
       "                      [-2.4079e-01, -1.7531e-01, -3.7780e-02, -1.9964e-01, -1.1101e-01,\n",
       "                        8.7113e-04, -9.1169e-03, -5.2960e-02, -1.9165e-02,  2.0004e-02,\n",
       "                       -3.4461e-02,  5.2700e-03, -6.4696e-03,  5.4672e-02, -7.8378e-02,\n",
       "                        7.6083e-02, -8.8314e-02, -3.7973e-01, -3.8861e-02, -1.1002e-01,\n",
       "                        1.2058e-02,  8.2219e-02, -2.8241e-01, -8.5590e-02, -2.3801e-01,\n",
       "                        5.7362e-02, -5.1579e-03,  1.6896e-01, -1.1233e-01, -3.7669e-02,\n",
       "                       -7.2150e-02, -7.8096e-02,  2.2766e-01,  5.1151e-02,  1.5774e-02,\n",
       "                       -7.2869e-02, -1.3265e-01, -3.0032e-02,  3.0327e-02, -3.4250e-01,\n",
       "                        1.5444e-03, -1.8356e-01,  2.3391e-02,  1.8282e-01,  5.1302e-02,\n",
       "                       -2.2133e-01, -7.8207e-02, -1.0980e-01,  2.3380e-01,  3.5705e-02,\n",
       "                       -9.3650e-02, -2.4679e-03,  3.3889e-02,  2.0977e-01,  1.4994e-01,\n",
       "                       -1.9264e-02, -9.0852e-02, -2.8840e-02, -1.0299e-01,  2.2239e-02,\n",
       "                        2.0740e-02,  1.2818e-02,  4.4960e-02, -9.1526e-02,  1.2790e-01,\n",
       "                       -6.4656e-02,  4.9279e-02, -1.4775e-01, -9.3201e-02, -8.3569e-03,\n",
       "                       -7.9899e-02,  6.3830e-02,  8.2029e-02, -2.1769e-01, -1.9629e-01,\n",
       "                       -1.1012e-01, -8.7280e-03,  3.5053e-02, -8.5262e-02, -1.2156e-01,\n",
       "                       -3.6493e-01, -3.9684e-02,  3.2892e-02,  1.4992e-02,  1.7620e-01,\n",
       "                       -3.0099e-02, -1.4655e-01, -6.4779e-02, -1.8409e-01,  2.4269e-01,\n",
       "                       -2.7224e-02,  1.4977e-01, -2.4714e-01, -6.6142e-02, -2.9413e-02,\n",
       "                       -1.2327e-02,  7.0812e-02, -6.6396e-02, -6.1269e-02, -3.9487e-02,\n",
       "                       -5.9394e-03, -7.9908e-02, -1.2220e-01,  8.3558e-02, -1.6006e-01,\n",
       "                       -9.3761e-02, -5.9244e-03, -1.7372e-01, -9.5410e-02,  4.6574e-02,\n",
       "                        4.6662e-02,  4.5288e-02, -3.6554e-01, -1.9169e-01,  4.1937e-02,\n",
       "                       -2.1069e-01, -6.9574e-02, -1.6814e-02, -1.4583e-01,  6.4361e-02,\n",
       "                       -7.5733e-02, -7.6471e-02, -5.8707e-02,  8.4214e-02,  2.1620e-02,\n",
       "                        1.8340e-01,  2.1717e-02, -1.1425e-01],\n",
       "                      [-9.2489e-03,  5.9175e-03, -4.8028e-02,  1.2071e-01, -1.4851e-01,\n",
       "                       -4.3441e-02, -3.9637e-02,  6.3319e-02,  1.1344e-01, -6.8181e-04,\n",
       "                        3.6965e-02,  3.1978e-04, -5.3269e-02,  4.0267e-02, -1.5321e-01,\n",
       "                        1.0207e-02, -1.1479e-01,  4.6328e-02,  1.6077e-02, -8.4341e-02,\n",
       "                        2.3526e-01, -1.4783e-01, -1.1569e-02,  2.6608e-03,  9.8418e-02,\n",
       "                       -1.6636e-02,  1.9352e-01, -9.3368e-02, -1.3805e-01, -1.6039e-01,\n",
       "                       -1.4973e-02,  1.3315e-01, -2.7505e-01, -5.1618e-02,  3.0915e-02,\n",
       "                        7.0450e-02,  1.3434e-01, -1.1322e-01,  3.1430e-02, -4.2359e-02,\n",
       "                       -4.3172e-02, -1.3055e-01,  1.0319e-01, -1.7192e-01,  9.3992e-03,\n",
       "                        4.7694e-02,  1.4040e-01,  1.1048e-01,  1.1369e-01,  1.4035e-02,\n",
       "                       -5.4129e-02, -6.3664e-02,  3.6627e-02,  2.2037e-02,  3.3792e-03,\n",
       "                       -4.0833e-02, -6.9103e-02,  1.0729e-01, -1.6219e-01,  1.4692e-01,\n",
       "                        9.0996e-02, -1.8388e-02, -1.1595e-02,  1.0485e-01, -8.7054e-02,\n",
       "                       -6.3628e-02, -3.9761e-02, -1.5317e-01,  1.2541e-01, -3.0566e-02,\n",
       "                        4.4505e-02, -7.6321e-02, -2.5963e-01, -1.3849e-01, -1.1502e-01,\n",
       "                        1.2341e-01, -1.5320e-02, -2.8083e-02, -1.5003e-01,  1.1107e-01,\n",
       "                        1.9646e-02, -6.0657e-02,  7.7926e-03, -4.7404e-02,  2.0863e-02,\n",
       "                        2.4531e-03, -2.0299e-02,  3.2087e-02, -5.3822e-02,  9.9014e-02,\n",
       "                       -1.6081e-02, -2.9062e-01, -6.4199e-03,  1.5015e-01, -6.0290e-02,\n",
       "                        3.2372e-03, -6.9470e-02,  1.2700e-02, -5.3687e-02, -8.0051e-02,\n",
       "                       -5.4196e-02, -6.2212e-02, -4.0308e-02, -1.7504e-01,  8.0858e-02,\n",
       "                        2.2056e-02,  5.0084e-03,  1.1950e-01,  1.4777e-01, -6.1297e-02,\n",
       "                        3.8873e-02,  3.7817e-02, -5.0158e-02, -1.1254e-01,  3.7401e-02,\n",
       "                       -1.5212e-01, -7.5908e-02, -4.0576e-03,  8.4270e-02,  1.6921e-01,\n",
       "                        3.0335e-02, -8.0909e-03,  4.0516e-02,  2.8162e-02, -5.2338e-02,\n",
       "                       -2.0759e-01,  3.0844e-03,  1.1153e-01],\n",
       "                      [ 3.8684e-02,  8.2152e-02, -1.7159e-01, -1.3756e-01,  9.4320e-02,\n",
       "                       -2.9234e-02, -4.5583e-02,  1.7426e-01,  6.2415e-02,  1.1198e-03,\n",
       "                        6.5906e-02, -2.2776e-02,  5.2537e-02, -1.3004e-02,  2.2963e-02,\n",
       "                       -3.7584e-02,  9.3725e-02,  4.6210e-02, -1.9706e-01,  1.1103e-01,\n",
       "                       -1.2092e-01, -1.4872e-01, -4.4550e-02,  1.1821e-01,  1.3059e-01,\n",
       "                       -2.5166e-02, -1.8033e-01, -2.5122e-01,  1.3095e-01, -1.2823e-01,\n",
       "                        1.9363e-02, -1.0510e-01, -1.1515e-01, -2.1286e-02,  1.1965e-01,\n",
       "                        3.4972e-02, -9.6941e-03, -1.7085e-02,  3.2361e-02, -7.9172e-02,\n",
       "                       -3.7282e-02,  1.2435e-01,  1.1652e-01, -1.5914e-01, -4.0059e-02,\n",
       "                       -6.5076e-02, -2.5750e-02, -6.7418e-02, -5.9686e-02,  1.4271e-02,\n",
       "                       -1.0419e-01, -3.2951e-02,  3.4691e-02, -1.4613e-01, -1.5475e-01,\n",
       "                       -3.1193e-02,  1.4018e-01, -4.0745e-03,  1.7418e-02, -2.3698e-01,\n",
       "                        9.1851e-02, -6.6742e-02,  9.1247e-02, -5.2401e-02,  1.2100e-01,\n",
       "                       -5.7942e-02,  5.0692e-02,  9.5709e-02,  9.8234e-02, -3.6925e-02,\n",
       "                        1.5464e-01,  4.7068e-02, -2.4455e-01,  1.2106e-01, -4.8860e-02,\n",
       "                        1.1559e-03, -3.4981e-03, -1.2321e-02, -1.1954e-01, -6.9218e-02,\n",
       "                        3.0794e-02, -2.5963e-02, -1.6650e-02,  1.9470e-02, -1.0620e-01,\n",
       "                        8.3681e-03,  1.0301e-01, -5.8143e-02,  4.0829e-02,  4.2927e-03,\n",
       "                        7.2720e-03, -9.2864e-02, -1.0249e-01, -7.0597e-02, -5.1205e-02,\n",
       "                       -1.4694e-01, -7.6006e-02,  5.5836e-02,  1.0739e-02, -6.3205e-02,\n",
       "                       -2.2961e-01, -6.3725e-02,  1.2783e-01,  3.2950e-02,  1.2045e-01,\n",
       "                        4.9162e-02,  6.5894e-03,  4.8708e-02, -1.1824e-01, -2.8330e-02,\n",
       "                        7.4220e-02, -1.0728e-02, -8.3526e-02,  2.1069e-02,  4.3581e-02,\n",
       "                        1.5104e-02, -3.8610e-03, -2.4131e-02,  1.0848e-01,  1.1241e-01,\n",
       "                        1.1194e-02, -3.9978e-02,  7.4559e-02, -1.4671e-01, -4.5962e-02,\n",
       "                       -2.1321e-01,  1.8824e-02, -1.3839e-01]])),\n",
       "             ('fc2.bias', tensor([-0.0108, -0.0445, -0.0734, -0.0732]))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0582cf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(f'{save_dir}/my_model3.pt', map_location=DEVICE)\n",
    "model2 = MyNet(n_vocab=VOCAB_SIZE)\n",
    "model2 = model2.to(DEVICE)\n",
    "model2.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ec03d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_loss, tst_acc = evaluate(model=model2, \n",
    "                             data_loader=tst_loader, \n",
    "                             criterion=loss_func,\n",
    "                             optimizer=my_opt, \n",
    "                             device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d107f6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tst Loss: 0.324 | Tst Acc: 89.645% \n"
     ]
    }
   ],
   "source": [
    "print(f'Tst Loss: {tst_loss:.3f} | Tst Acc: {100*tst_acc:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64e060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
